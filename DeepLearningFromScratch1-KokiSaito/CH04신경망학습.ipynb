{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7275fbd-ea71-4643-ad81-8d8b525733db",
   "metadata": {},
   "source": [
    "# CH04 신경망 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da80bebc-72d8-418f-af8d-8aece14b5a49",
   "metadata": {},
   "source": [
    "## 4.1 데이터에서 학습한다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba4450b-559b-401a-a7a8-33ff0a405995",
   "metadata": {},
   "source": [
    "**학습** : 훈련데이터로부터 가중치 매개변수의 최적값을 자동으로 획득하는 것  \n",
    "**손실함수** : 신경망이 학습할 수 있도록하는 지표  \n",
    "기계학습은 인간의 개입을 최대한 배제라고 숨은 규칙성을 파악하는 데에 중점을 둠 \n",
    "  \n",
    "접근법  \n",
    "* **(고전)머신러닝** : 사람이 생각한 특징을 svm등의 모델에 먹여 학습\n",
    "* **신경망(딥러닝)** : 기계가 알아서 데이터를 관찰하고 숨겨진 특성 찾음\n",
    "  \n",
    "머신러닝은 범용성을 확보하기 위해 데이터를 훈련데이터와 시럼데이터로 나눔  \n",
    "**범용성이란? 수집하진 못한 데이터에도 적용이 돼야함**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dc1277-fa83-47dd-bcae-88c69e7f6195",
   "metadata": {},
   "source": [
    "## 4.2 손실함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c107ba0-df2c-486c-89e3-ea902ad294e2",
   "metadata": {},
   "source": [
    "**손실함수**: 신경망 성능의 나쁨을 나타내는 지표로 이 지표를 낮추는 것이 머신러닝의 과제 (음수처리하여 얼마나 좋은지에 대한 지표로도 사용가능)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfae417d-efc7-4498-a3d7-24c29675cb62",
   "metadata": {},
   "source": [
    "**오차제곱합**: Sum Squard Error  \n",
    "말그대로 오차를 제곱해서 시그마 때린 것  \n",
    "$$ E = \\frac{1}{2}\\sum_{k}(y_k-t_k)^2 $$\n",
    "$$ y_k = 신경망의 추정 값$$\n",
    "$$ t_k = 정답레이블 $$\n",
    "$$k = 데이터의 차원 수(쉽게 말해 출력층의 노드 수)$$\n",
    "\n",
    "구현 시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7c6b0de-87de-43fe-b4a3-8125e72d9691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_squared_error(y,t):\n",
    "    return 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5934ef1-de5a-4254-a7f5-c50779377f2c",
   "metadata": {},
   "source": [
    "**교차엔트로피 오차**  \n",
    "CrossEntropyError (CEE)  \n",
    "주로 분류에 사용  \n",
    "$$ E = -\\sum_{k}t_klog(y_k) $$\n",
    "$$ t_k = \\text{정답레이블} $$\n",
    "$$ y_k = \\text{모델예측값} $$\n",
    "$$ k = \\text{출력층의 노드 수} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192f9c7d-10f8-4f48-80ae-a57f314824e4",
   "metadata": {},
   "source": [
    "**미니배치학습을 활용하여 미니배치에CEE 적용하기**  \n",
    "미니배치 : 훈련 데이터에 대한 손실함수를 모든데이터에 대해 구하기 힘들때 **데이터의 일부를 추려 하나의 데이터처럼 이용** (Batch의 개념)  \n",
    "즉, 데이터 일부를 추려 전체의 근사치로 활용하는 것  \n",
    "$$E = - \\frac{1}{N} \\sum_{n} \\sum_{k} t_{nk} \\log y_{nk}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4544b94d-cc3d-4227-8967-f13dba3c1023",
   "metadata": {},
   "source": [
    "| 구분 | Step 1 (입력) | Step 2 (오차 계산) | Step 3 (학습) | Step 4 (반복) |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| **CEE** | 데이터 하나 넣음<br>`(1 x 784)` | 망에 넣어 오차 얻음 | 가중치 수정 | 다음 데이터 넣고 반복 |\n",
    "| **미니배치 CEE** | 데이터 배치 넣음<br>`(N x 784)` | 망에 넣어 CEE 오차들을 얻고<br>평균내서 새 오차 얻음 | 가중치 수정 | 다음 배치 넣고 반복 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdcdfbef-152e-4513-b9c3-e9c65cdf3ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from dataset.mnist import load_mnist\n",
    "from PIL import Image\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    load_mnist(flatten = True, normalize = True, one_hot_label=True)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9bd3d47-d345-47b5-be85-9d3cea24aaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y,t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1,t.size)\n",
    "        y = y.reshape(1,y.size)\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(mp.lof(y[np.arange(batch_size),t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8651a6-5df3-4ee2-8e2c-4808ae6cef1f",
   "metadata": {},
   "source": [
    "## 4.3 수치미분"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5ae7fa-9e4f-4697-9e36-d5c7cc236fba",
   "metadata": {},
   "source": [
    "$ \\frac{f(x + h) - f(x)}{h} $ 로 미분 구현 시 두가지 문제 내포\n",
    "* 반올림 오차 : 작은 값 무시하므로 h->0 하기 힘듦\n",
    "* 차분 : 점$x+h$ 와 점$x$ 사이의 기울기를 구한 것이지 미분계수를 구한 것이 아니기에 오차가 생김(h->0 구현의 한계)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06be834-f40d-4861-9334-381c6d03f636",
   "metadata": {},
   "source": [
    "**중심 차분**으로 오차를 줄임  \n",
    "$ \\frac{f(x + h) - f(x - h)}{2h} $  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4a9e486-8b89-42ca-a23e-ba46da666e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_difference(f,x):\n",
    "    h = 1e-4\n",
    "    return (f(x+h) - f(x-h)) / (2*h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc1b206a-9324-43bf-9463-d415b22ddb51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1999999999990898"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#수치 미분의 예\n",
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x\n",
    "\n",
    "numerical_difference(function_1, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b19611-681a-4aa7-b81d-7691c831123f",
   "metadata": {},
   "source": [
    "편미분  \n",
    "$ f(x_0,x_1) = x_{0}^2 + x_{1}^2 $의 식일 때 변수가 2개이므로 주의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c7cbd7d-3353-4ecb-8a26-20008bf8fd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2+x[1]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b44cd66-951c-4209-a65b-35dba17372ce",
   "metadata": {},
   "source": [
    "#### 함수 사용 예문"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1096c5-c65e-46e0-9c37-d27da1e5121d",
   "metadata": {},
   "source": [
    "문제:  \n",
    "$ x_0 = 3, x_1 = 4 $ 일때, $ x_0 $에 대한 편미분을 구하여라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8618c8b3-30fb-4daf-801e-8b3423fe0ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.00000000000378"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1 : 상수 고정\n",
    "def function_tmp1(x0): \n",
    "    return x0*x0 + 4.0**2.0\n",
    "\n",
    "# Step 2 : x0을 기준으로 미분 진행\n",
    "numerical_difference(function_tmp1,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77562fa5-d656-4a42-808c-84c95cf80690",
   "metadata": {},
   "source": [
    "문제:  \n",
    "$ x_0 = 3, x_1 = 4 $ 일때, $ x_1 $에 대한 편미분을 구하여라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0954ccd6-dca4-4cdb-a5a3-06352d97bbbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.999999999999119"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1 : 상수 고정\n",
    "def function_tmp2(x1): \n",
    "    return 3.0**2.0 + x1*x1\n",
    "\n",
    "# Step 2 : x0을 기준으로 미분 진행\n",
    "numerical_difference(function_tmp2,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45f2db4-ac19-4cef-9bd5-14667feaa599",
   "metadata": {},
   "source": [
    "## 4.4 기울기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d194da5c-2a12-4776-9733-8060e57e2cca",
   "metadata": {},
   "source": [
    "4.3에서는 $x_0, x_1$ 각각에 대해 변수별로 따졌었음  \n",
    "하지만 실제로는 편미분을 동시에 계산하는것이 효율적  \n",
    "이때, $(\\frac{\\partial f}{\\partial x_0},\\frac{\\partial f}{\\partial x_1})$ 처럼 모든 변수의 편미분을 벡터로 정리한 것을 **기울기**라고함  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0d930e0-6eb6-427a-a69c-fca40457eacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f,x): #f에는 함수 x에는 array 받음\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        #f(x+h)계산\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "\n",
    "        grad[idx] = (fxh1 - fxh2) / (2 * h)\n",
    "        x[idx] = tmp_val\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3e73845-6e65-47c9-b6a3-58b0fbf4d81e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 8.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "numerical_gradient(function_2, np.array([3.0,4.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be7feaf-febc-47f4-959b-80d92d198c3b",
   "metadata": {},
   "source": [
    "이 기울기들은 함수의(적어도 해당 부분에서의) 가장 낮은 값을 가리킴  \n",
    "즉, 기울기가 가리키는 방향은 각 장소에 함구의 출력값을 가장 크게 줄이는 방향  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01767f2-3575-480c-8434-083dd69af1b3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### **Gemini 활용 보충 이해**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c504704-367b-4557-829e-c200d937af5b",
   "metadata": {},
   "source": [
    "1. 상상해보기: 밥그릇 모양 산먼저 이 함수의 생김새를 머릿속에 그려야 합니다.$$z = x_{0}^2 + x_{1}^2$$ 그래프는 아래쪽이 둥근 '밥그릇' 모양입니다.가장 낮은 지점(목표): 바닥인 $(0, 0)$ 지점입니다. (높이 0)현재 위치: 우리가 밥그릇의 벽면 어딘가인 $(3, 4)$ 지점에 서 있다고 가정해 봅시다.현재 높이$$(손실값): 3^2 + 4^2 = 9 + 16 = \\mathbf{25}$$\n",
    "2. 기울기 계산 (나침반 만들기)이제 여기서 미분을 합니다. 변수가 2개니까 편미분을 해야겠죠?$x_0$ 방향 기울기: $\\frac{\\partial f}{\\partial x_0} = 2x_0$현재 위치 $x_0=3$ 대입 $\\rightarrow$ $6$의미: \"동쪽($x_0$)으로 한 발짝 가면 높이가 6만큼 가파르게 올라간다.\"$x_1$ 방향 기울기: $\\frac{\\partial f}{\\partial x_1} = 2x_1$현재 위치 $x_1=4$ 대입 $\\rightarrow$ $8$의미: \"북쪽($x_1$)으로 한 발짝 가면 높이가 8만큼 아주 가파르게 올라간다.\"기울기 벡터 (Gradient):이 두 숫자를 합쳐서 괄호로 묶으면 그게 바로 기울기 벡터입니다.$$(\\frac{\\partial f}{\\partial x_0}, \\frac{\\partial f}{\\partial x_1}) = (6, 8)$$\n",
    "3. \"방향이 있다\"는 뜻 해석자, 이제 **$(6, 8)$**이라는 숫자가 나왔습니다. 이걸 좌표평면에 그려볼까요?가로($x_0$)로 6칸, 세로($x_1$)로 8칸 가는 화살표를 그립니다.이 화살표는 **원점 $(0,0)$의 정반대 방향(바깥쪽)**을 가리키고 있습니다.이 그림을 보면 화살표들이 전부 **\"밥그릇 바깥쪽(높아지는 쪽)\"**을 향해 뻗어 나가고 있죠? 이게 바로 기울기의 방향입니다.기울기 벡터 $(6, 8)$의 진짜 의미:\"지금 서 있는 $(3,4)$ 지점에서 $(6, 8)$ 방향(북동쪽)으로 움직이면 산을 가장 빠르게 올라갈 수 있다.\"4. \"가장 낮은 값을 가리킨다\"는 말의 속뜻여기서 작성자님이 질문하신 문장의 비밀이 풀립니다.기울기 방향 $(6, 8)$: 함수값이 가장 크게 증가하는(산 위로 가는) 방향.기울기의 반대 방향 $(-6, -8)$: 함수값이 가장 크게 감소하는(산 아래로 가는) 방향.책에서 \"기울기가 가장 낮은 곳을 가리킨다\"라고 표현한 것은, 우리가 학습을 할 때 기울기에 마이너스($-$)를 붙여서 반대 방향으로 가기 때문입니다.수식으로 보면 명확합니다.$$x_{new} = x_{old} - \\eta \\frac{\\partial f}{\\partial x}$$현재 위치 $(3, 4)$기울기 $(6, 8)$ $\\rightarrow$ 산 위쪽 방향빼기($-$): $(3, 4) - (6, 8) = (-3, -4)$ $\\rightarrow$ 산 아래쪽(원점) 방향!즉, **\"기울기(벡터) 자체는 오르막길 방향을 가리키지만, 우리는 그 정보를 이용해서 내리막길(가장 낮은 값)을 찾아낸다\"** 는 뜻입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15986986-bdab-4985-95c6-4ff54076105f",
   "metadata": {},
   "source": [
    "### 경사하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d4c19f-5903-4a20-ac04-4f39b2473ead",
   "metadata": {},
   "source": [
    "학습단계에서 최적의 매개변수(가중치와 편향)을 찾아 내야함(여기서 최적이란 손실함수가 최솟값이 될때의 매개변수 값)  \n",
    "**경사법**은 그 시점의 위치에서 기울어진 방향으로 일정거리만큼 이동을 하고 계산을 반복하며 함수의 값이 최소가 되는 찾아나아가는 과정임  \n",
    "하지만 **기울어진 방향이 꼭 최솟값을 가리키는 것은 아님**으로 주의해야함  \n",
    "경사법의 방식은 기울기가 0이 되는 지점을 찾아나아가는 것이므로  \n",
    "최솟값, **안장점(saddle point)** 처럼 기울기가 0인 지점을 찾게 되면 그곳이 반드시 최솟값임을 보장할 수 없음  \n",
    "실제로 대부분의 복잡한 함수는 기울기가 가리키는 방향에 최솟값이 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d98304-6420-4e42-8716-b18bed43add1",
   "metadata": {},
   "source": [
    "경사법을 수식으로 나타내면 다음과 같음\n",
    "$$x_{0} = x_{0} - \\eta \\frac{\\partial f}{\\partial x}$$\n",
    "$$x_{1} = x_{1} - \\eta \\frac{\\partial f}{\\partial x}$$\n",
    "$ \\eta \\text{  에타}^{eta} $기호는 갱신하는 양을 나타냄  \n",
    "이를 신경망 학습에서 **학습률**이라고 부름  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0bda09-a974-4522-8b1c-769a2d13b29a",
   "metadata": {},
   "source": [
    "특징  \n",
    "* 변수의 수가 늘어나도 같은 식을 사용하여 갱신\n",
    "* 학습률은 미리 정해 두어야함 (0.01, 0.001등의 작은 값)\n",
    "* 학습률을 변경해가며 올바르게 학습하고 있는지 확인하며 진행하며 이 값이 너무 크거나 작으면 최적의 장소를 찾지 못함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e25e7fb9-b690-49cb-ba3f-7c71505884c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr=0.01,step_num = 100):\n",
    "    x = init_x\n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f,x) #앞서 작성한 편미분 식\n",
    "        x -= lr*grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c226ca06-24a3-45f8-85de-0e1e3838d389",
   "metadata": {},
   "source": [
    "f는 최적화하려는 함수, init_x는 초깃값(추후 기울기가 될 것), lr은 learning rate(학습률), step_num은 경삿법 반복횟수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0325f866-6353-4524-8f06-00d4d43fe6dd",
   "metadata": {},
   "source": [
    "#### 경사법 예문"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bdafc3-481c-467e-b0b8-e3336716f312",
   "metadata": {},
   "source": [
    "경사법으로 $ f(x_0,x_1) = x_0^2 + x_1^2 $를 구하여라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a47cd9aa-ff4f-4e61-b5ca-5d82edc5f1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2+x[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6f30e6b7-710d-4479-8bc8-60124685258e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.04890207e-09, -7.40505637e-09])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_x = np.array([3.0,-4.4])\n",
    "gradient_descent(function_2,init_x = init_x, lr = 0.01, step_num = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990ff97e-fb2c-4e11-bb4f-7363072c707e",
   "metadata": {},
   "source": [
    "이 값은 0,0에 매우 가까운 값으로 실제로 진정한 최솟값이 0,0이르모 정확한 결과 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb1e900-81ef-40ae-856b-1a1e82148289",
   "metadata": {},
   "source": [
    "학습률이 너무 크거나 작은 경우 잘 작동 하지 않음 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6094c0e-fb41-49e8-8b2f-0ad421fd999e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습률 너무 클 때 : [2.58985795e+13 1.49815380e+12]\n",
      "학습률 너무 작을 때 : [2.58985795e+13 1.49815380e+12]\n"
     ]
    }
   ],
   "source": [
    "init_x = np.array([3.0,-4.4])\n",
    "print(\"학습률 너무 클 때 :\",gradient_descent(function_2,init_x = init_x, lr = 10, step_num = 1000))\n",
    "print(\"학습률 너무 작을 때 :\", gradient_descent(function_2,init_x = init_x, lr = 1e-10, step_num = 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e62e92-480f-48bd-8278-658f04a22abb",
   "metadata": {},
   "source": [
    "엄청나게 큰 값으로 발산해버림"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3fe05d-a2e0-4cf1-9b96-18a9e8f88e04",
   "metadata": {},
   "source": [
    "### 신경망에서의 기울기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3b2501-10ec-4270-9ca3-97f7e505d042",
   "metadata": {},
   "source": [
    "신경망에서는 **가중치 매개변수**에 대한 **손실함수**의 기울기를 구해야함  \n",
    "$\\frac{\\partial L}{\\partial \\boldsymbol{W}}$ 의 각 원소에 관한 편미분은 다음과 같음  \n",
    "$$Gradient = \\frac{\\partial L}{\\partial W} \\text{  (L: LossFunction, W: Weight)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb6c99c-9c4a-48d0-ba5e-86c375e9aa2b",
   "metadata": {},
   "source": [
    "$$W = \\begin{pmatrix} \n",
    "W_{11} & W_{12} & W_{13} \\\\ \n",
    "W_{21} & W_{22} & W_{23} \n",
    "\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b277bf57-fb81-496e-a1d3-dee93f3ec87a",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial L}{\\partial W} = \\begin{pmatrix} \n",
    "\\dfrac{\\partial L}{\\partial W_{11}} & \\dfrac{\\partial L}{\\partial W_{12}} & \\dfrac{\\partial L}{\\partial W_{13}} \\\\ \n",
    "\\dfrac{\\partial L}{\\partial W_{21}} & \\dfrac{\\partial L}{\\partial W_{22}} & \\dfrac{\\partial L}{\\partial W_{23}} \n",
    "\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14632c20-5bba-457c-8bb2-84195ab4804d",
   "metadata": {},
   "source": [
    "이때 1행의 첫번 째 원소는 $W_{11}$을 조금 변경했을 때 손실함수 $L$이 얼마나 변하느냐를 나타냄"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d984ee84-ae15-402b-99d1-fe823614a2db",
   "metadata": {},
   "source": [
    "$\\frac{\\partial L}{\\partial \\boldsymbol{W}}$의 형상이 $\\boldsymbol{W}$와 같으므로 연산가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a040e9-3ca8-41fa-9244-6e9a983c9f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3)\n",
    "\n",
    "    def predict(self,x):\n",
    "        return np.dot(x,self.W)\n",
    "\n",
    "    def loss(self,x,t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y,t)\n",
    "        return loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TorchResearch",
   "language": "python",
   "name": "torchresearch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
