{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7275fbd-ea71-4643-ad81-8d8b525733db",
   "metadata": {},
   "source": [
    "# CH04 신경망 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da80bebc-72d8-418f-af8d-8aece14b5a49",
   "metadata": {},
   "source": [
    "## 4.1 데이터에서 학습한다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba4450b-559b-401a-a7a8-33ff0a405995",
   "metadata": {},
   "source": [
    "**학습** : 훈련데이터로부터 가중치 매개변수의 최적값을 자동으로 획득하는 것  \n",
    "**손실함수** : 신경망이 학습할 수 있도록하는 지표  \n",
    "기계학습은 인간의 개입을 최대한 배제라고 숨은 규칙성을 파악하는 데에 중점을 둠 \n",
    "  \n",
    "접근법  \n",
    "* **(고전)머신러닝** : 사람이 생각한 특징을 svm등의 모델에 먹여 학습\n",
    "* **신경망(딥러닝)** : 기계가 알아서 데이터를 관찰하고 숨겨진 특성 찾음\n",
    "  \n",
    "머신러닝은 범용성을 확보하기 위해 데이터를 훈련데이터와 시럼데이터로 나눔  \n",
    "**범용성이란? 수집하진 못한 데이터에도 적용이 돼야함**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dc1277-fa83-47dd-bcae-88c69e7f6195",
   "metadata": {},
   "source": [
    "## 4.2 손실함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c107ba0-df2c-486c-89e3-ea902ad294e2",
   "metadata": {},
   "source": [
    "**손실함수**: 신경망 성능의 나쁨을 나타내는 지표로 이 지표를 낮추는 것이 머신러닝의 과제 (음수처리하여 얼마나 좋은지에 대한 지표로도 사용가능)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfae417d-efc7-4498-a3d7-24c29675cb62",
   "metadata": {},
   "source": [
    "**오차제곱합**: Sum Squard Error  \n",
    "말그대로 오차를 제곱해서 시그마 때린 것  \n",
    "$$ E = \\frac{1}{2}\\sum_{k}(y_k-t_k)^2 $$\n",
    "$$ y_k = 신경망의 추정 값$$\n",
    "$$ t_k = 정답레이블 $$\n",
    "$$k = 데이터의 차원 수(쉽게 말해 출력층의 노드 수)$$\n",
    "\n",
    "구현 시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7c6b0de-87de-43fe-b4a3-8125e72d9691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_squared_error(y,t):\n",
    "    return 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5934ef1-de5a-4254-a7f5-c50779377f2c",
   "metadata": {},
   "source": [
    "**교차엔트로피 오차**  \n",
    "CrossEntropyError (CEE)  \n",
    "주로 분류에 사용  \n",
    "$$ E = -\\sum_{k}t_klog(y_k) $$\n",
    "$$ t_k = \\text{정답레이블} $$\n",
    "$$ y_k = \\text{모델예측값} $$\n",
    "$$ k = \\text{출력층의 노드 수} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192f9c7d-10f8-4f48-80ae-a57f314824e4",
   "metadata": {},
   "source": [
    "**미니배치학습을 활용하여 미니배치에CEE 적용하기**  \n",
    "미니배치 : 훈련 데이터에 대한 손실함수를 모든데이터에 대해 구하기 힘들때 **데이터의 일부를 추려 하나의 데이터처럼 이용** (Batch의 개념)  \n",
    "즉, 데이터 일부를 추려 전체의 근사치로 활용하는 것  \n",
    "$$E = - \\frac{1}{N} \\sum_{n} \\sum_{k} t_{nk} \\log y_{nk}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4544b94d-cc3d-4227-8967-f13dba3c1023",
   "metadata": {},
   "source": [
    "| 구분 | Step 1 (입력) | Step 2 (오차 계산) | Step 3 (학습) | Step 4 (반복) |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| **CEE** | 데이터 하나 넣음<br>`(1 x 784)` | 망에 넣어 오차 얻음 | 가중치 수정 | 다음 데이터 넣고 반복 |\n",
    "| **미니배치 CEE** | 데이터 배치 넣음<br>`(N x 784)` | 망에 넣어 CEE 오차들을 얻고<br>평균내서 새 오차 얻음 | 가중치 수정 | 다음 배치 넣고 반복 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdcdfbef-152e-4513-b9c3-e9c65cdf3ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from dataset.mnist import load_mnist\n",
    "from PIL import Image\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = \\\n",
    "    load_mnist(flatten = True, normalize = True, one_hot_label=True)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9bd3d47-d345-47b5-be85-9d3cea24aaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y,t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1,t.size)\n",
    "        y = y.reshape(1,y.size)\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(mp.lof(y[np.arange(batch_size),t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8651a6-5df3-4ee2-8e2c-4808ae6cef1f",
   "metadata": {},
   "source": [
    "## 4.3 수치미분"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5ae7fa-9e4f-4697-9e36-d5c7cc236fba",
   "metadata": {},
   "source": [
    "$ \\frac{f(x + h) - f(x)}{h} $ 로 미분 구현 시 두가지 문제 내포\n",
    "* 반올림 오차 : 작은 값 무시하므로 h->0 하기 힘듦\n",
    "* 차분 : 점$x+h$ 와 점$x$ 사이의 기울기를 구한 것이지 미분계수를 구한 것이 아니기에 오차가 생김(h->0 구현의 한계)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06be834-f40d-4861-9334-381c6d03f636",
   "metadata": {},
   "source": [
    "**중심 차분**으로 오차를 줄임  \n",
    "$ \\frac{f(x + h) - f(x - h)}{2h} $  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4a9e486-8b89-42ca-a23e-ba46da666e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_difference(f,x):\n",
    "    h = 1e-4\n",
    "    return (f(x+h) - f(x-h)) / (2*h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc1b206a-9324-43bf-9463-d415b22ddb51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1999999999990898"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#수치 미분의 예\n",
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x\n",
    "\n",
    "numerical_difference(function_1, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b19611-681a-4aa7-b81d-7691c831123f",
   "metadata": {},
   "source": [
    "편미분  \n",
    "$ f(x_0,x_1) = x_{0}^2 + x_{1}^2 $의 식일 때 변수가 2개이므로 주의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c7cbd7d-3353-4ecb-8a26-20008bf8fd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2+x[1]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b44cd66-951c-4209-a65b-35dba17372ce",
   "metadata": {},
   "source": [
    "#### 함수 사용 예문"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1096c5-c65e-46e0-9c37-d27da1e5121d",
   "metadata": {},
   "source": [
    "문제:  \n",
    "$ x_0 = 3, x_1 = 4 $ 일때, $ x_0 $에 대한 편미분을 구하여라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8618c8b3-30fb-4daf-801e-8b3423fe0ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.00000000000378"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1 : 상수 고정\n",
    "def function_tmp1(x0): \n",
    "    return x0*x0 + 4.0**2.0\n",
    "\n",
    "# Step 2 : x0을 기준으로 미분 진행\n",
    "numerical_difference(function_tmp1,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77562fa5-d656-4a42-808c-84c95cf80690",
   "metadata": {},
   "source": [
    "문제:  \n",
    "$ x_0 = 3, x_1 = 4 $ 일때, $ x_1 $에 대한 편미분을 구하여라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0954ccd6-dca4-4cdb-a5a3-06352d97bbbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.999999999999119"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1 : 상수 고정\n",
    "def function_tmp2(x1): \n",
    "    return 3.0**2.0 + x1*x1\n",
    "\n",
    "# Step 2 : x0을 기준으로 미분 진행\n",
    "numerical_difference(function_tmp2,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45f2db4-ac19-4cef-9bd5-14667feaa599",
   "metadata": {},
   "source": [
    "## 4.4 기울기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d194da5c-2a12-4776-9733-8060e57e2cca",
   "metadata": {},
   "source": [
    "4.3에서는 $x_0, x_1$ 각각에 대해 변수별로 따졌었음  \n",
    "하지만 실제로는 편미분을 동시에 계산하는것이 효율적  \n",
    "이때, $(\\frac{\\partial f}{\\partial x_0},\\frac{\\partial f}{\\partial x_1})$ 처럼 모든 변수의 편미분을 벡터로 정리한 것을 **기울기**라고함  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0d930e0-6eb6-427a-a69c-fca40457eacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f,x): #f에는 함수 x에는 array 받음\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        #f(x+h)계산\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "\n",
    "        grad[idx] = (fxh1 - fxh2) / (2 * h)\n",
    "        x[idx] = tmp_val\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3e73845-6e65-47c9-b6a3-58b0fbf4d81e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 8.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "numerical_gradient(function_2, np.array([3.0,4.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be7feaf-febc-47f4-959b-80d92d198c3b",
   "metadata": {},
   "source": [
    "이 기울기들은 함수의(적어도 해당 부분에서의) 가장 낮은 값을 가리킴  \n",
    "즉, 기울기가 가리키는 방향은 각 장소에 함구의 출력값을 가장 크게 줄이는 방향  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01767f2-3575-480c-8434-083dd69af1b3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### **Gemini 활용 보충 이해**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c504704-367b-4557-829e-c200d937af5b",
   "metadata": {},
   "source": [
    "1. 상상해보기: 밥그릇 모양 산먼저 이 함수의 생김새를 머릿속에 그려야 합니다.$$z = x_{0}^2 + x_{1}^2$$ 그래프는 아래쪽이 둥근 '밥그릇' 모양입니다.가장 낮은 지점(목표): 바닥인 $(0, 0)$ 지점입니다. (높이 0)현재 위치: 우리가 밥그릇의 벽면 어딘가인 $(3, 4)$ 지점에 서 있다고 가정해 봅시다.현재 높이$$(손실값): 3^2 + 4^2 = 9 + 16 = \\mathbf{25}$$\n",
    "2. 기울기 계산 (나침반 만들기)이제 여기서 미분을 합니다. 변수가 2개니까 편미분을 해야겠죠?$x_0$ 방향 기울기: $\\frac{\\partial f}{\\partial x_0} = 2x_0$현재 위치 $x_0=3$ 대입 $\\rightarrow$ $6$의미: \"동쪽($x_0$)으로 한 발짝 가면 높이가 6만큼 가파르게 올라간다.\"$x_1$ 방향 기울기: $\\frac{\\partial f}{\\partial x_1} = 2x_1$현재 위치 $x_1=4$ 대입 $\\rightarrow$ $8$의미: \"북쪽($x_1$)으로 한 발짝 가면 높이가 8만큼 아주 가파르게 올라간다.\"기울기 벡터 (Gradient):이 두 숫자를 합쳐서 괄호로 묶으면 그게 바로 기울기 벡터입니다.$$(\\frac{\\partial f}{\\partial x_0}, \\frac{\\partial f}{\\partial x_1}) = (6, 8)$$\n",
    "3. \"방향이 있다\"는 뜻 해석자, 이제 **$(6, 8)$**이라는 숫자가 나왔습니다. 이걸 좌표평면에 그려볼까요?가로($x_0$)로 6칸, 세로($x_1$)로 8칸 가는 화살표를 그립니다.이 화살표는 **원점 $(0,0)$의 정반대 방향(바깥쪽)**을 가리키고 있습니다.이 그림을 보면 화살표들이 전부 **\"밥그릇 바깥쪽(높아지는 쪽)\"**을 향해 뻗어 나가고 있죠? 이게 바로 기울기의 방향입니다.기울기 벡터 $(6, 8)$의 진짜 의미:\"지금 서 있는 $(3,4)$ 지점에서 $(6, 8)$ 방향(북동쪽)으로 움직이면 산을 가장 빠르게 올라갈 수 있다.\"4. \"가장 낮은 값을 가리킨다\"는 말의 속뜻여기서 작성자님이 질문하신 문장의 비밀이 풀립니다.기울기 방향 $(6, 8)$: 함수값이 가장 크게 증가하는(산 위로 가는) 방향.기울기의 반대 방향 $(-6, -8)$: 함수값이 가장 크게 감소하는(산 아래로 가는) 방향.책에서 \"기울기가 가장 낮은 곳을 가리킨다\"라고 표현한 것은, 우리가 학습을 할 때 기울기에 마이너스($-$)를 붙여서 반대 방향으로 가기 때문입니다.수식으로 보면 명확합니다.$$x_{new} = x_{old} - \\eta \\frac{\\partial f}{\\partial x}$$현재 위치 $(3, 4)$기울기 $(6, 8)$ $\\rightarrow$ 산 위쪽 방향빼기($-$): $(3, 4) - (6, 8) = (-3, -4)$ $\\rightarrow$ 산 아래쪽(원점) 방향!즉, **\"기울기(벡터) 자체는 오르막길 방향을 가리키지만, 우리는 그 정보를 이용해서 내리막길(가장 낮은 값)을 찾아낸다\"** 는 뜻입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15986986-bdab-4985-95c6-4ff54076105f",
   "metadata": {},
   "source": [
    "### 경사하강법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d4c19f-5903-4a20-ac04-4f39b2473ead",
   "metadata": {},
   "source": [
    "학습단계에서 최적의 매개변수(가중치와 편향)을 찾아 내야함(여기서 최적이란 손실함수가 최솟값이 될때의 매개변수 값)  \n",
    "**경사법**은 그 시점의 위치에서 기울어진 방향으로 일정거리만큼 이동을 하고 계산을 반복하며 함수의 값이 최소가 되는 찾아나아가는 과정임  \n",
    "하지만 **기울어진 방향이 꼭 최솟값을 가리키는 것은 아님**으로 주의해야함  \n",
    "경사법의 방식은 기울기가 0이 되는 지점을 찾아나아가는 것이므로  \n",
    "최솟값, **안장점(saddle point)** 처럼 기울기가 0인 지점을 찾게 되면 그곳이 반드시 최솟값임을 보장할 수 없음  \n",
    "실제로 대부분의 복잡한 함수는 기울기가 가리키는 방향에 최솟값이 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d98304-6420-4e42-8716-b18bed43add1",
   "metadata": {},
   "source": [
    "경사법을 수식으로 나타내면 다음과 같음\n",
    "$$x_{0} = x_{0} - \\eta \\frac{\\partial f}{\\partial x}$$\n",
    "$$x_{1} = x_{1} - \\eta \\frac{\\partial f}{\\partial x}$$\n",
    "$ \\eta \\text{  에타}^{eta} $기호는 갱신하는 양을 나타냄  \n",
    "이를 신경망 학습에서 **학습률**이라고 부름  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0bda09-a974-4522-8b1c-769a2d13b29a",
   "metadata": {},
   "source": [
    "특징  \n",
    "* 변수의 수가 늘어나도 같은 식을 사용하여 갱신\n",
    "* 학습률은 미리 정해 두어야함 (0.01, 0.001등의 작은 값)\n",
    "* 학습률을 변경해가며 올바르게 학습하고 있는지 확인하며 진행하며 이 값이 너무 크거나 작으면 최적의 장소를 찾지 못함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e25e7fb9-b690-49cb-ba3f-7c71505884c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr=0.01,step_num = 100):\n",
    "    x = init_x\n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f,x) #앞서 작성한 편미분 식\n",
    "        x -= lr*grad\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c226ca06-24a3-45f8-85de-0e1e3838d389",
   "metadata": {},
   "source": [
    "f는 최적화하려는 함수, init_x는 초깃값(추후 기울기가 될 것), lr은 learning rate(학습률), step_num은 경삿법 반복횟수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0325f866-6353-4524-8f06-00d4d43fe6dd",
   "metadata": {},
   "source": [
    "#### 경사법 예문"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bdafc3-481c-467e-b0b8-e3336716f312",
   "metadata": {},
   "source": [
    "경사법으로 $ f(x_0,x_1) = x_0^2 + x_1^2 $를 구하여라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a47cd9aa-ff4f-4e61-b5ca-5d82edc5f1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2+x[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f30e6b7-710d-4479-8bc8-60124685258e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.04890207e-09, -7.40505637e-09])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_x = np.array([3.0,-4.4])\n",
    "gradient_descent(function_2,init_x = init_x, lr = 0.01, step_num = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990ff97e-fb2c-4e11-bb4f-7363072c707e",
   "metadata": {},
   "source": [
    "이 값은 0,0에 매우 가까운 값으로 실제로 진정한 최솟값이 0,0이르모 정확한 결과 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb1e900-81ef-40ae-856b-1a1e82148289",
   "metadata": {},
   "source": [
    "학습률이 너무 크거나 작은 경우 잘 작동 하지 않음 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6094c0e-fb41-49e8-8b2f-0ad421fd999e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습률 너무 클 때 : [2.58985795e+13 1.49815380e+12]\n",
      "학습률 너무 작을 때 : [2.58985795e+13 1.49815380e+12]\n"
     ]
    }
   ],
   "source": [
    "init_x = np.array([3.0,-4.4])\n",
    "print(\"학습률 너무 클 때 :\",gradient_descent(function_2,init_x = init_x, lr = 10, step_num = 1000))\n",
    "print(\"학습률 너무 작을 때 :\", gradient_descent(function_2,init_x = init_x, lr = 1e-10, step_num = 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e62e92-480f-48bd-8278-658f04a22abb",
   "metadata": {},
   "source": [
    "엄청나게 큰 값으로 발산해버림"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3fe05d-a2e0-4cf1-9b96-18a9e8f88e04",
   "metadata": {},
   "source": [
    "### 신경망에서의 기울기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3b2501-10ec-4270-9ca3-97f7e505d042",
   "metadata": {},
   "source": [
    "신경망에서는 **가중치 매개변수**에 대한 **손실함수**의 기울기를 구해야함  \n",
    "$\\frac{\\partial L}{\\partial \\boldsymbol{W}}$ 의 각 원소에 관한 편미분은 다음과 같음  \n",
    "$$Gradient = \\frac{\\partial L}{\\partial W} \\text{  (L: LossFunction, W: Weight)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb6c99c-9c4a-48d0-ba5e-86c375e9aa2b",
   "metadata": {},
   "source": [
    "$$W = \\begin{pmatrix} \n",
    "W_{11} & W_{12} & W_{13} \\\\ \n",
    "W_{21} & W_{22} & W_{23} \n",
    "\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b277bf57-fb81-496e-a1d3-dee93f3ec87a",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial L}{\\partial W} = \\begin{pmatrix} \n",
    "\\dfrac{\\partial L}{\\partial W_{11}} & \\dfrac{\\partial L}{\\partial W_{12}} & \\dfrac{\\partial L}{\\partial W_{13}} \\\\ \n",
    "\\dfrac{\\partial L}{\\partial W_{21}} & \\dfrac{\\partial L}{\\partial W_{22}} & \\dfrac{\\partial L}{\\partial W_{23}} \n",
    "\\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14632c20-5bba-457c-8bb2-84195ab4804d",
   "metadata": {},
   "source": [
    "이때 1행의 첫번 째 원소는 $W_{11}$을 조금 변경했을 때 손실함수 $L$이 얼마나 변하느냐를 나타냄"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d984ee84-ae15-402b-99d1-fe823614a2db",
   "metadata": {},
   "source": [
    "$\\frac{\\partial L}{\\partial \\boldsymbol{W}}$의 형상이 $\\boldsymbol{W}$와 같으므로 연산가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b53418-d2c7-4595-9647-99c52fbe78e3",
   "metadata": {},
   "source": [
    "이때 신경망을 구현하면"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24952d9f-af05-4be8-a69c-e5c52ed9e2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys,os\n",
    "import numpy as np\n",
    "sys.path.append(os.pardir)\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3)\n",
    "\n",
    "    def predict(self,x):\n",
    "        return np.dot(x,self.W)\n",
    "\n",
    "    def loss(self,x,t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y,t)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae79d892-10f8-4898-b936-17f9ff46aa0d",
   "metadata": {},
   "source": [
    "2*3 가중치 매개변수 하나를 인스턴스 변수로 갖게함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1495e8d8-0ab8-4563-bb90-72c15a39f52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.61039102 -1.16813497  0.78142122]\n",
      " [ 0.37635363  0.66122515  1.16761228]]\n"
     ]
    }
   ],
   "source": [
    "net = simpleNet()\n",
    "print(net.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fa186a7-9bc0-46de-8e93-722ae7eea828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.62968215 -0.23802338  1.28618133]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0.6, 0.7])\n",
    "p = net.predict(x)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a3e8ff-df31-4ff7-8691-37de97b83d37",
   "metadata": {},
   "source": [
    "p는 입력과 가중치만을 곱한 날것의 점수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d66dae2b-ebfa-477b-b4b5-c16f1e8a09d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.5518472957793161)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=np.array([0,0,1])\n",
    "net.loss(x,t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f052294-9c91-4876-a2c2-636f86f90674",
   "metadata": {},
   "source": [
    "메서드 내부에서 p의 점수가 softmax를 통과하며 점수(합쳐서1이 되는 값)으로 변환 되고  \n",
    "이를 CEE가 받아 다시 오차를 계산 해줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44d4b033-6327-421c-a25b-7abed633b03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.1792144   0.07525462 -0.25446902]\n",
      " [ 0.20908346  0.08779706 -0.29688052]]\n"
     ]
    }
   ],
   "source": [
    "def f(W):\n",
    "    return net.loss(x,t)\n",
    "\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd818cc8-9748-4f81-bbbf-35b5b886b927",
   "metadata": {},
   "source": [
    "각 숫자는 W의 값을 학습률 만큼 올리면 어떻게 변하는지를 보여줌"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0b2191-caa3-4d60-878b-febfe46b68c7",
   "metadata": {},
   "source": [
    "## 4.5 학습알고리즘 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ffc096-0fa3-40cb-8ab3-1fc2bc574957",
   "metadata": {},
   "source": [
    "신경망 학습의 Step  \n",
    "전제  \n",
    "신경망에는 적응 가능한 가중치와 편향이 있고, 가중치와 편향을 훈련 데이터에 맞게 적절히 조정하는 과정을 학습이라 부름. 신경망 학습은 다음 Step을 따름\n",
    "1. 미니배치  \n",
    "훈련데이터 중 일부를 무작위로 가져옴. (이때 선별한 데이터를 미니배치라 하며 미니배치의 손실함수 값을 줄이는 것이 목표)\n",
    "2. 기울기산출  \n",
    "미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구함\n",
    "3. 매개변수갱신  \n",
    "가중치 매개변수를 기울기 방향으로 아주 조금 갱신\n",
    "4. 반복\n",
    "1~3단계 반복"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fe475e-6f94-4b05-8db2-540dfa8c5b93",
   "metadata": {},
   "source": [
    "이때 미니배치를 무작위로 선정하기 때문에 **확률적경사하강법(SGD)** 이라부름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4300b20-27e6-4560-9263-c4ebef039bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self,input_size,hidden_size,\n",
    "                 output_size,weight_init_std=0.01):\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size,output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1,W2 = self.params['W1'], self.params['W2']\n",
    "        b1,b2 = self.params['b1'], self.params['b2']\n",
    "\n",
    "        a1 = np.dot(x,W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1,W2) + b2\n",
    "        y = softmax(a2)\n",
    "\n",
    "        return y\n",
    "\n",
    "    def loss(self,x,t):\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y,t)\n",
    "\n",
    "    def accuracy(self,x,t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis = 1)\n",
    "        t = np.argmax(t, axis = 1)\n",
    "\n",
    "        accuracy = np.sum(y==t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    def numerical_gradient(self,x,t):\n",
    "        loss_W = lambda W: self.loss(x,t)\n",
    "\n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ef6bd7-a361-42f6-b3e7-503c813484f7",
   "metadata": {},
   "source": [
    "편미분 시 데이터의 흐름 :\n",
    "1. numerical_gradient(loss_W, self.params['W1'])에서 x[idx] = tmp_val +- h 연산이 일어남\n",
    "2. 이는 주소값이 직접 전달된것이므로 이때 가중치가 각각 실제로 변함\n",
    "3. 변한 가중치에 대해, 입력값인 x,t를 사용하여, 손실값이 만들어내는 차이와 미분값을 구함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18c23e6-2acc-443b-92be-821bafa6a22d",
   "metadata": {},
   "source": [
    "헷갈렸던 점:  \n",
    "numerical_gradient(f,x)꼴에서 손실함수(f)가 loss_W = lambda W: self.loss(x,t)이기에 loss_W는 self.params['W1']을 직접적으로 받지 않는다. 헌데 어떻게 numerical_gradient는 f에 자신의 x를 전달하는가?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ec56e1-7b02-4c2c-a858-f7d2c048eb30",
   "metadata": {},
   "source": [
    "해결:  \n",
    "grads['W1'] = numerical_gradient(loss_W, self.params['W1'])에서 self.params['W1']는 loss_W에 직접 전달되는 값이 아닌 내부의 가중치들을 임시적으로 변환시키기 위해 사용  \n",
    "손실함수는 입력된 값에 대해 이 내부의 변화를 반영하여 손실함수를 계산  \n",
    "결과적으로numerical_gradient는 손실함수가 계산해준 값을 2개 받아서 차이를 구함으로서 각 가중치에 대한 편미분값을 구함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ec0841d-7e7c-43c7-8ba0-60ce3d978323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1: (784, 100)\n",
      "b1: (100,)\n",
      "W2: (100, 10)\n",
      "b2: (10,)\n"
     ]
    }
   ],
   "source": [
    "net = TwoLayerNet(input_size=784,hidden_size=100,output_size=10)\n",
    "print(\"W1:\",net.params['W1'].shape)\n",
    "print(\"b1:\",net.params['b1'].shape)\n",
    "print(\"W2:\",net.params['W2'].shape)\n",
    "print(\"b2:\",net.params['b2'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127c1cd9-c864-4b14-8575-1c2dd5753c54",
   "metadata": {},
   "source": [
    "### **미니배치학습 구현**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9019f7b7-de71-4161-97d2-b45b266015a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "(x_train, t_train),(x_test,t_test) = load_mnist(normalize = True, one_hot_label=True)\n",
    "\n",
    "train_loss_list = []\n",
    "\n",
    "iters_num = 10000 #반복 수\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "network = TwoLayerNet(input_size=784)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TorchResearch",
   "language": "python",
   "name": "torchresearch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
